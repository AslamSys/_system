# ğŸ‘¥ Speaker ID / Diarization

**Container:** `speaker-id-diarization`  
**Ecossistema:** Mordomo  
**PosiÃ§Ã£o no Fluxo:** Quinto - identificaÃ§Ã£o de falantes

## ğŸš€ Quick Start

### InstalaÃ§Ã£o

```bash
# Instalar dependÃªncias de teste
cd test_data
pip install -r requirements.txt
```

### Criar Embeddings de Teste

```bash
# UsuÃ¡rio 1 (vocÃª)
python test_data/create_embedding.py user_1

# UsuÃ¡rio 2 (outra pessoa)
python test_data/create_embedding.py user_2
```

### Testar Diarization

```bash
# Gravar Ã¡udio com mÃºltiplos falantes e testar separaÃ§Ã£o
python test_data/test_diarization.py --duration 10
```

Consulte `test_data/README.md` para detalhes completos dos testes.

---

## ğŸ“‹ PropÃ³sito

Identificar QUEM estÃ¡ falando em cada momento (com embeddings cadastrados), separar conversas por falante e detectar sobreposiÃ§Ã£o de vozes.

**Processamento Paralelo:** Inicia anÃ¡lise junto com Whisper ASR apÃ³s `wake_word.detected`, mas sÃ³ envia metadados downstream apÃ³s `speaker.verified` (GATE). Reduz latÃªncia total em ~100-200ms.

**ğŸ”¬ TECNICAMENTE:** Combina Diarization (separar vozes) + Speaker Recognition (identificar com embeddings):
- **Diarization puro** sÃ³ agrupa: "Speaker 0, Speaker 1" (anÃ´nimos)
- **Speaker Recognition** identifica: compara embedding atual vs database cadastrado â†’ "user_1", "user_2" ou "unknown"
- **HÃ­brido** = separa vozes + identifica cada uma com user_id especÃ­fico

**ğŸ”’ CRÃTICO PARA SEGURANÃ‡A:** Re-autentica continuamente durante conversaÃ§Ã£o ativa, prevenindo escalaÃ§Ã£o de privilÃ©gios quando falante muda (ex: admin inicia sessÃ£o, convidado tenta comando privilegiado).

---

## ğŸ¯ Responsabilidades

- âœ… **Iniciar anÃ¡lise** apÃ³s `wake_word.detected` (paralelo com Verification)
- âœ… **Receber Ã¡udio + transcriÃ§Ã£o** do Whisper ASR (gRPC)
- âœ… **Processar em buffer** atÃ© receber `speaker.verified` (GATE)
- âœ… **Identificar falante** (user_1, user_2, guest_123, etc)
- âœ… **Re-autenticar continuamente** (prevenir escalaÃ§Ã£o de privilÃ©gios)
- âœ… **Detectar troca de falante** durante conversaÃ§Ã£o ativa
- âœ… **Detectar sobreposiÃ§Ã£o** de vozes
- âœ… **Publicar metadados** apÃ³s gate aberto
- âœ… **Trigger Source Separation** se necessÃ¡rio
- âœ… **Descartar buffer** se `speaker.rejected`

---

## ğŸ”„ Estados do Speaker ID

```
IDLE (aguardando)
  â†“ wake_word.detected
BUFFERING (processa mas nÃ£o publica)
  â”œâ”€ speaker.verified â†’ ANALYZING
  â””â”€ speaker.rejected â†’ IDLE (descarta buffer)
ANALYZING (publica resultados)
  â†“ conversation.ended (reseta gate)
IDLE (limpa contexto, pronto para prÃ³xima)
```

**OtimizaÃ§Ã£o:** Processa em paralelo com Verification (~200ms), resultados prontos quando gate abre.
**Reset:** `conversation.ended` limpa todo contexto e volta ao IDLE, resetando o gate para prÃ³xima detecÃ§Ã£o.

---

## ğŸ”§ Tecnologias

**Linguagem:** Python (obrigatÃ³rio - ecossistema ML)

**Principal:** pyannote.audio (HÃ­brido: Diarization + Recognition)
- Diarization state-of-the-art
- Speaker embeddings (256D)
- Overlap detection
- **Recognition:** Compara com embeddings cadastrados
- **Backend:** PyTorch (C++ libtorch nativo)

**Modelo:** `pyannote/speaker-diarization-3.1` + embeddings

**Database Compartilhado:**
- Usa MESMOS embeddings do Speaker Verification
- Embeddings cadastrados: `/data/embeddings/user_1.npy`, `/data/embeddings/user_2.npy`
- Threshold: 0.70 (mais permissivo que Verification)

**Alternativas:**
- Resemblyzer (mesma tech do Verification)
- SpeechBrain Speaker Recognition (PyTorch)
- Custom model (TensorFlow Lite)

**Performance:** Diarization model em PyTorch C++, embedding comparison em NumPy C (OpenBLAS). Python overhead ~10ms.

---

## ğŸ’¾ Armazenamento de Embeddings (Compartilhado)

### Volume Compartilhado com Speaker Verification

```yaml
# Estrutura de diretÃ³rios
./data/embeddings/  (host - criado pelo Verification)
  â””â”€ /data/embeddings/  (container - bind mount RO)

MESMOS arquivos do Verification:
  â”œâ”€ user_1.npy
  â”œâ”€ user_2.npy
  â””â”€ guest_*.npy
```

**Modo de Acesso:** Read-Only (RO)
- Speaker Verification: Read-Write (cria/atualiza embeddings)
- Speaker ID/Diarization: Read-Only (apenas lÃª)

**SincronizaÃ§Ã£o:**
- Embeddings criados pelo Verification sÃ£o automaticamente visÃ­veis
- Hot reload automÃ¡tico (watchdog detecta novos arquivos)
- LatÃªncia de leitura: ~0.5ms (cache do kernel)

**Docker Compose:**
```yaml
services:
  speaker-id-diarization:
    volumes:
      - ./data/embeddings:/data/embeddings:ro  # Read-Only
    environment:
      - EMBEDDINGS_PATH=/data/embeddings
```

**Vantagens:**
- âœ… Zero duplicaÃ§Ã£o de dados
- âœ… ConsistÃªncia automÃ¡tica (mesma fonte)
- âœ… LatÃªncia desprezÃ­vel (< 1% do total)
- âœ… Cadastro centralizado (enrollment via Verification)

---

## ğŸ“Š EspecificaÃ§Ãµes

```yaml
Input:
  Audio + Transcript
  Sample Rate: 16000 Hz
  Duration: variÃ¡vel

Diarization:
  Min Speaker Duration: 1.0s
  Overlap Detection: true
  Max Speakers: 3
  
Recognition (identificaÃ§Ã£o com embeddings):
  Database: /data/embeddings/*.npy (compartilhado com Verification)
  Enrolled Users: 2+ (user_1, user_2, guests)
  Threshold: 0.70 (cosine similarity)
  Unknown Detection: true
  Embedding Dimension: 256D
  
Performance:
  CPU: 30-50% spike
  RAM: ~ 800 MB
  Latency: < 300 ms
  Accuracy: > 90%
```

---

## ğŸ”Œ Interfaces

### Input (gRPC)
```protobuf
message DiarizeRequest {
  bytes audio = 1;
  string transcript = 2;
  int64 timestamp = 3;
}
```

### Output (NATS)
```python
# Voz reconhecida (cadastrada)
subject: "speech.diarized.{speaker_id}"
payload: {
  "text": "qual a temperatura",
  "speaker_id": "user_1",  # CRÃTICO: usado para re-autenticaÃ§Ã£o
  "recognized": true,       # Voz encontrada no database
  "confidence": 0.88,       # Similaridade com embedding cadastrado
  "start_time": 0.5,
  "end_time": 2.3,
  "overlap_detected": false,
  "timestamp": 1732723200.123,
  "conversation_id": "abc123"
}

# Voz desconhecida (nÃ£o cadastrada)
subject: "speech.diarized.unknown"
payload: {
  "text": "desliga o alarme",
  "speaker_id": "unknown_abc123",  # Hash do embedding
  "recognized": false,              # âš ï¸ Voz NÃƒO encontrada
  "confidence": 0.42,               # Melhor match (< 0.70 threshold)
  "start_time": 5.0,
  "end_time": 7.5,
  "overlap_detected": false,
  "timestamp": 1732723205.123,
  "conversation_id": "abc123"
}

# Conversation Manager usa para:
#  1. Validar permissÃµes do FALANTE ATUAL (nÃ£o do dono da sessÃ£o)
#  2. IGNORAR comandos de recognized=false (vozes desconhecidas)
#  3. Detectar troca de falante (anti-escalaÃ§Ã£o)
#  4. Manter contexto individualizado
#  5. Log de auditoria
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

```yaml
diarization:
  model: "pyannote/speaker-diarization-3.1"
  min_speaker_duration: 1.0
  max_speakers: 3
  
recognition:
  # Database compartilhado com Speaker Verification
  embeddings_path: "/data/embeddings"
  threshold: 0.70  # Cosine similarity
  unknown_detection: true
  
  # UsuÃ¡rios cadastrados
  enrolled_users:
    - user_id: "user_1"
      embedding_file: "user_1.npy"
    - user_id: "user_2"
      embedding_file: "user_2.npy"
  
overlap:
  detection_enabled: true
  threshold: 0.5  # SobreposiÃ§Ã£o temporal
  
source_separation:
  trigger_on_overlap: true
  min_overlap_duration: 0.5  # segundos

nats:
  url: "nats://nats:4222"
  publish_recognized: "speech.diarized.{speaker_id}"
  publish_unknown: "speech.diarized.unknown"
```

---

## ğŸ”¬ ImplementaÃ§Ã£o TÃ©cnica

### Fluxo HÃ­brido: Diarization + Recognition

```python
class SpeakerIdentifier:
    def __init__(self):
        # Carrega embeddings cadastrados (mesmo database do Verification)
        self.enrolled_embeddings = {
            "user_1": np.load("/data/embeddings/user_1.npy"),
            "user_2": np.load("/data/embeddings/user_2.npy")
        }
        self.threshold = 0.70
    
    async def identify_and_diarize(self, audio, transcript):
        # 1. DIARIZATION: Separa segmentos por voz
        diarization = self.diarize(audio)
        # Output: [(0s-5s, speaker_0), (6s-10s, speaker_1)]
        
        results = []
        for segment in diarization:
            start, end, speaker_cluster = segment
            audio_segment = audio[start:end]
            
            # 2. RECOGNITION: Identifica quem Ã©
            embedding = self.create_embedding(audio_segment)
            
            # Compara com cadastrados
            similarities = {}
            for user_id, enrolled_emb in self.enrolled_embeddings.items():
                sim = cosine_similarity(embedding, enrolled_emb)
                similarities[user_id] = sim
            
            best_match = max(similarities.items(), key=lambda x: x[1])
            user_id, confidence = best_match
            
            # 3. Valida threshold
            if confidence >= self.threshold:
                recognized = True
            else:
                user_id = f"unknown_{hash(embedding)}"
                recognized = False
            
            # 4. Publica resultado
            results.append({
                "text": transcript[start:end],
                "speaker_id": user_id,
                "confidence": confidence,
                "recognized": recognized,
                "start_time": start,
                "end_time": end
            })
        
        return results
```

### DiferenÃ§a: Diarization Puro vs HÃ­brido

```python
# âŒ Diarization Puro (anÃ´nimo)
Output: [
  {"speaker": "Speaker_0", "text": "qual a temperatura"},  # Quem Ã©?
  {"speaker": "Speaker_1", "text": "desliga o alarme"}     # Quem Ã©?
]
# Problema: NÃ£o sabe QUEM Ã© cada speaker!

# âœ… HÃ­brido (com Recognition)
Output: [
  {"speaker_id": "user_1", "recognized": true, "text": "qual a temperatura"},
  {"speaker_id": "unknown_xyz", "recognized": false, "text": "desliga o alarme"}
]
# SoluÃ§Ã£o: Identifica user_id especÃ­fico OU unknown
```

---

## ğŸ“ˆ MÃ©tricas

```python
speaker_identifications_total{speaker_id}  # user_1, user_2, unknown
speaker_recognition_success_rate           # recognized=true / total
speaker_unknown_detections_total           # recognized=false
speaker_overlap_detections_total
speaker_diarization_latency_seconds
speaker_confidence_avg{speaker_id}
source_separation_triggers_total
```

---

## ğŸ”— IntegraÃ§Ã£o

**Recebe de:** Whisper ASR (gRPC - Ã¡udio + texto)  
**Envia para:** Conversation Manager (NATS - speech.diarized)  
**Compartilha:** Embeddings com Speaker Verification (volume `/data/embeddings`)  
**Monitora:** Prometheus, Loki

---

**VersÃ£o:** 1.0
