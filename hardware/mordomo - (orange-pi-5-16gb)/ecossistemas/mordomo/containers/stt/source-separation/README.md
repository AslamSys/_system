# ğŸµ Source Separation

**Container:** `source-separation`  
**Ecossistema:** Mordomo  
**PosiÃ§Ã£o no Fluxo:** Opcional/Condicional - separaÃ§Ã£o de vozes

---

## ğŸ“‹ PropÃ³sito

Separar vozes sobrepostas quando duas pessoas falam simultaneamente, melhorando precisÃ£o do STT e Speaker ID.

---

## ğŸ¯ Responsabilidades

- âœ… Ativado APENAS quando Speaker ID detecta overlap
- âœ… Separar Ã¡udio em canais por falante
- âœ… Reenviar Ã¡udio separado para STT refinado
- âœ… NÃ£o afetar latÃªncia em conversas normais

---

## ğŸ”§ Tecnologias

**Linguagem:** Python (obrigatÃ³rio - PyTorch)

**Principal:** Demucs
- SeparaÃ§Ã£o de sources (vozes, mÃºsica, etc)
- VersÃ£o lightweight para ARM
- **Backend:** PyTorch (C++ libtorch nativo)

**Modelo:** `htdemucs_ft` (fine-tuned para voz)

**Performance:** Model inference em PyTorch C++, processamento intensivo de CPU. Python overhead desprezÃ­vel (<1% do total 1-3s).

---

## ğŸ“Š EspecificaÃ§Ãµes

```yaml
Input:
  Audio com overlap
  Duration: 1-5 segundos
  Sample Rate: 16000 Hz

Separation:
  Sources: 2-3 vozes
  Quality: Alta
  
Performance:
  CPU: 60-80% spike (intensivo!)
  RAM: ~ 1.5 GB
  Latency: 1-3 segundos
  Uso: < 5% do tempo (apenas quando overlap)
```

---

## ğŸ”Œ Interfaces

### Input (NATS)
```python
# Enviado por: Speaker ID quando detecta overlap_detected=true
subject: "audio.overlap_detected"
payload: {
  "audio": "<base64 encoded PCM>",
  "duration": 2.5,
  "speakers": ["user_1", "user_2"],
  "conversation_id": "uuid",
  "timestamp": 1732723200.123
}
```

### Output (NATS)
```python
# Envia Ã¡udio separado de volta para Whisper reprocessar
subject: "audio.separated"
payload: {
  "channels": [
    {
      "audio": "<base64 PCM channel 1>",
      "speaker_id": "user_1",
      "confidence": 0.85
    },
    {
      "audio": "<base64 PCM channel 2>",
      "speaker_id": "user_2",
      "confidence": 0.78
    }
  ],
  "conversation_id": "uuid",
  "original_duration": 2.5,
  "timestamp": 1732723201.456
}

# Whisper ASR subscreve audio.separated e retranscribe
# Nova transcriÃ§Ã£o â†’ Speaker ID â†’ speech.diarized (com overlap resolvido)
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

```yaml
demucs:
  model: "htdemucs_ft"
  device: "cpu"
  shifts: 1
  overlap: 0.25
  
processing:
  max_duration: 5.0  # segundos
  batch_size: 1
  num_workers: 2
  
trigger:
  min_overlap_duration: 0.5
  confidence_threshold: 0.6
```

---

## ğŸ“ˆ MÃ©tricas

```python
source_separation_requests_total
source_separation_latency_seconds
source_separation_success_total
source_separation_quality_score
```

---

## ğŸ”— IntegraÃ§Ã£o

### Fluxo Completo de Overlap

```
1. Whisper transcribe â†’ Speaker ID (Ã¡udio + texto)
   â†“
2. Speaker ID detecta overlap (2+ vozes simultÃ¢neas)
   â””â”€ NATS: audio.overlap_detected
   â†“
3. Source Separation recebe e processa (1-3s)
   â””â”€ Demucs separa em canais (channel 1, channel 2)
   â†“
4. Source Separation publica: NATS audio.separated
   â†“
5. Whisper subscreve audio.separated
   â””â”€ Retranscribe cada canal separadamente
   â†“
6. Whisper â†’ Speaker ID (nova transcriÃ§Ã£o refinada)
   â””â”€ Agora sem overlap, identifica corretamente
   â†“
7. Speaker ID â†’ NATS: speech.diarized (overlap resolvido)
```

**Recebe de:** Speaker ID (NATS `audio.overlap_detected`)  
**Envia para:** Whisper ASR (NATS `audio.separated`)  
**Monitora:** Prometheus

---

**âš ï¸ Nota:** Container pesado (60-80% CPU, 1.5GB RAM) - usar apenas quando necessÃ¡rio (<5% do tempo)

---

**VersÃ£o:** 1.0
